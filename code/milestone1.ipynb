{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing Data (Milestone 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(fname):\n",
    "\n",
    "    # Separate file into books with all metadata\n",
    "    startBook = \"<book\"\n",
    "    books = []\n",
    "    with open(fname) as openFile:\n",
    "        for line in openFile:\n",
    "            if line[:len(startBook)] == startBook:\n",
    "                books.append(line)\n",
    "            else:\n",
    "                books[-1] += line\n",
    "    openFile.close()\n",
    "\n",
    "    books = np.array(books)\n",
    "\n",
    "    # Collect synopses and all genre tags for each book\n",
    "    # X_data found here (synopses)\n",
    "    synopses = []\n",
    "    genres = []\n",
    "\n",
    "    startBody = \"<body>\"\n",
    "    endBody = \"</body>\"\n",
    "\n",
    "    startTopic = \"<topics>\"\n",
    "    endTopic = \"</topics>\"\n",
    "\n",
    "    for book in books:\n",
    "        start = book.index(startBody) + len(startBody)\n",
    "        end = book.index(endBody)\n",
    "        synopses.append(book[start:end])\n",
    "\n",
    "        start = book.index(startTopic) + len(startTopic)\n",
    "        end = book.index(endTopic)\n",
    "        genres.append(book[start:end])\n",
    "\n",
    "    X_data = np.array(synopses)\n",
    "\n",
    "\n",
    "    # Create matrix of genre tags to book\n",
    "    # Y_data found here\n",
    "    start = \">\"\n",
    "    end = \"</\"\n",
    "\n",
    "    Y_data = np.zeros((len(books), len(allGenres)))\n",
    "\n",
    "    for i in range(len(books[:200])):\n",
    "        entry = genres[i]\n",
    "        while entry.find(end) != -1:\n",
    "            s = entry.index(start) + 1\n",
    "            e = entry.index(end)\n",
    "            genreTag = entry[s:e]\n",
    "            Y_data[i][allGenres.index(genreTag)] = 1\n",
    "            entry = entry[e+5:]\n",
    "\n",
    "    return (X_data, Y_data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of all genre tags\n",
    "allGenres = []\n",
    "with open(\"../data/hierarchy.txt\") as hierarchyFile:\n",
    "    for line in hierarchyFile:\n",
    "        tab = line.find(\"\\t\")\n",
    "        g1 = line[:tab]\n",
    "        g2 = line[tab+1:len(line)-1]\n",
    "        if allGenres.count(g1) == 0:\n",
    "            allGenres.append(g1)\n",
    "        if allGenres.count(g2) == 0:\n",
    "            allGenres.append(g2)\n",
    "\n",
    " # print(allGenres[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = clean(\"../data/BlurbGenreCollection_EN_train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' print(X_train[0])\\nprint(Y_train[0])\\nprint(np.where(Y_train[0]==1))\\nprint(genres[0])\\nprint(allGenres.index(\"Nonfiction\"))\\nprint(allGenres.index(\"Games\"))  '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" print(X_train[0])\n",
    "print(Y_train[0])\n",
    "print(np.where(Y_train[0]==1))\n",
    "print(genres[0])\n",
    "print(allGenres.index(\"Nonfiction\"))\n",
    "print(allGenres.index(\"Games\"))  \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "X_test, Y_test = clean(\"../data/BlurbGenreCollection_EN_test.txt\")\n",
    "X_valid, Y_valid = clean(\"../data/BlurbGenreCollection_EN_dev.txt\")\n",
    "\n",
    "# print(X_test[0])\n",
    "# print(Y_test[0])\n",
    "\n",
    "# print(X_valid[0])\n",
    "# print(Y_valid[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the Model and Training! (Milestone 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Activation\n",
    "from tensorflow.keras.layers import Flatten, Conv1D, MaxPool1D\n",
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.losses import BinaryCrossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'seq_length' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/xt/ccmc0dfs37qb76b_klc6g67c0000gn/T/ipykernel_68862/1277703198.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfilter_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m model.add(Conv1D(filters=nb_filters, kernel_size=filter_size, activation='sigmoid', \n\u001b[0;32m----> 5\u001b[0;31m                  kernel_initializer='glorot_normal', input_shape=(seq_length, 1)))\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMaxPool1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m model.add(Conv1D(filters=nb_filters, kernel_size=filter_size, activation='tanh', \n",
      "\u001b[0;31mNameError\u001b[0m: name 'seq_length' is not defined"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# DEFINE seq_length as length of longest blurb...maybe?\n",
    "nb_filters=16\n",
    "filter_size=3\n",
    "model.add(Conv1D(filters=nb_filters, kernel_size=filter_size, activation='sigmoid', \n",
    "                 kernel_initializer='glorot_normal', input_shape=(seq_length, 1)))\n",
    "model.add(MaxPool1D()) \n",
    "model.add(Conv1D(filters=nb_filters, kernel_size=filter_size, activation='sigmoid', \n",
    "                 kernel_initializer='glorot_normal'))\n",
    "model.add(Conv1D(filters=nb_filters, kernel_size=filter_size, activation='sigmoid', \n",
    "                 kernel_initializer='glorot_normal'))\n",
    "model.add(MaxPool1D()) \n",
    "model.add(Conv1D(filters=nb_filters, kernel_size=filter_size, activation='sigmoid', \n",
    "                 kernel_initializer='glorot_normal'))\n",
    "model.add(MaxPool1D()) \n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='BinaryCrossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', \n",
    "                   patience=50, verbose=1,\n",
    "                   restore_best_weights=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X,Y, batch_size=32, validation_split=0.2, callbacks=[es], epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-18 21:26:10.570553: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict(X_test)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
